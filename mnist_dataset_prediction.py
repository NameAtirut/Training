# -*- coding: utf-8 -*-
"""MNIST dataset Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ofy4YmlShqq9Y0-KH_tcguvyZe7eB-VX

## MNIST dataset
Using a Custom model of Convolutional NN

Import the dataset
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers

(x_train, y_train), (x_test,y_test)= tf.keras.datasets.mnist.load_data(path="mnist.npz")

"""Explore and Prepare the data"""

def insight(data):
  print(f"Min: {data.min()}")
  print(f"Max: {data.max()}")
  print(f"Mean:{data.mean()} ")
  print("\n")

# Check the overall value of the dataset
insight1 = list(map(insight,(x_train,x_test)))
print(insight1)

"""Transformation"""

# Scale the inputs down to (0,1)
x_train = x_train.astype("float32") / x_train.max()
x_test = x_test.astype("float32") / x_test.max()

insight2 = list(map(insight,(x_train,x_test)))
print(insight2)

# from (60000,28,28) to (60000,28,28,1)
import numpy as np
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

# Number of samples in each set
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")

# convert the labels from Integer class to Binary class (for crossentropy validation)
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

"""### Build the CNN model


*   Sequential (feed-forward)
*   2 hidden layers of 32 and 64 filters of (3,3)
  -   Activation function: Rectify Linear Unit 
  -   Pooling (2,2): reduce the output size of each layer by 1/4


*   Dropout: decreases overfitting
*   Output layer: SoftMax Act.Fn.





"""

input_shape = (28,28,1)
# Sequential : feed-forward CNN model
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
     # Two CNN layers with 32/64 filters of 3x3 size and Pooling to reduce the size of output
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
     
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
     # Flatten to 1D
        layers.Flatten(),
     # Set dropout rate to avoid overfitting
        layers.Dropout(0.5),
     # Output layer
        layers.Dense(num_classes, activation="softmax"),
    ]
)

model.summary()

"""### Compile & Train

*   Loss fn. = categorical_crossentropy
*   Optimizer = adam : Gradient Descent Algorithm
*   Output = accuracy


"""

batch_size = 128
epochs = 15

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

"""Evaluate the model"""

score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""Visualise the results"""

import matplotlib.pyplot as plt
x_test1, xtrain1 = map(np.squeeze,(x_test,x_train))
plt.imshow(x_test1[0])

COLOR = 'Blue'
plt.rcParams['text.color'] = COLOR
plt.rcParams['axes.labelcolor'] = COLOR

def show_image(img, label, guess):
  plt.figure()
  plt.imshow(img)
  plt.title(f"Expected: {label}")
  plt.xlabel(f"Guess: {guess}")
  plt.colorbar()
  plt.grid(False)
  plt.show()


def get_number():
  while True:
    num = input("Pick a number: ")
    if num.isdigit():
      num = int(num)
      if 0 <= num <= 1000:
        return int(num)
    else:
      print("Try again...")

num = get_number()
image = x_test1[num]
label = y_test[num]

class_names = ['0','1','2','3','4','5','6','7','8','9']
prediction = model.predict(np.array([x_test[num]]))
predicted_class = class_names[np.argmax(prediction)]

show_image(image, np.argmax(label), predicted_class)